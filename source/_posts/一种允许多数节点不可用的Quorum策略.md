---
title: 一种允许多数节点不可用的Quorum策略
date: 2021-10-11 15:10:04
tags:
- 一致性算法
categories: 编程
comments: true
---

*本文探讨了一种Quorum策略，可以在大多数节点失效的情况下达成一致。以及对这种Quorum策略的可用性讨论。*
<!--more-->

# 1 背景

在《JPacificA - 一种跨域线性一致性算法》中，我们提出了一种支持跨域传输的线性一致性算法。但是这个算法只是将跨域问题进行了分割，将数据的跨域一致性分成了元数据的一致性和业务数据的一致性两个问题，并且解决了业务数据一致性的问题。而对于元数据，我们在此文中提到的各种跨域部署集群的问题依然存在，只是规模被限制在了一定范围内。

集群被跨地域部署，我们在JpacificA的文章中提到了核心问题有

> 写入响应时间长、选举时间长

而当故障发生时，这个问题会被显著的放大。比如两地三中心的情况下集群按照3-2-2部署，总共7个节点，至少要4个节点达成一致。当本地的节点宕机2个或以上时，由于要和异地节点达成一致，响应时间会被放大几十乃至上百倍（取决于同城机房与异地机房之间相隔距离的倍数）。

例如我们假设两地三中心进行部署(以北京上海为例)，北京同城两个数据中心的的延迟为1ms，北京到上海的延迟大约为30ms。

此时如果发生同时发生2个节点故障，假设每个节点故障概率相等，那么如果需要故障后集群写入或者选举的响应时间不增加，则故障机器不能同时在北京的机器中，所以有
$$
A_5^2 / A_7^2 = 0.476
$$
的概率响应时间会从1ms上升为30ms。
			

# 2 Quorum

在现有的一致性算法中，通常对Quorum的定义为“一半以上的节点”。由于两个由一半以上节点组成的集合必然有交集，而同一个任期中每个节点又至多投出一票，以此保证正确性。

但是Quorum定义为“一半以上的节点”并不是一个共识算法保证正确性的充要条件。只要Quorum策略能保证任意两个合法的Quorum集合相交后不为空集，即能保证数据的一致性。由此，我们针对跨地域部署的集群，定义以下Quorum策略：

**假设存在若干地域,编号为1到m。某个地域i(1<=i<=m)任意一半以上的节点组成的集合为Qi。所有可能的Q1-Qm 中，选取任意k（m/2<k<=m）个集合，组成的并集即为Quorum。**

例如我们假设进行三地部署(以北京上海杭州为例)，假设每个数据中心各有3个节点，其中上海的节点称为s1、s2、s3，杭州的节点称为h1、h2、h3,北京的节点称为b1 、b2、b3。那么由至少2个地域，每个地域有至少2个节点组成的集合，就是我们的合法Quorum。由定义可知，Quorum中至少需要存在4个节点。对于4个节点组成Quorum的场景，总共就有
$$
A_3^2 * A_3^2 = 36
$$
种可能。

在以上的数据集合中，任意两个集合必有交集，以此保证算法的正确性。

# 3 应用

假设北京到杭州的延迟约为40ms，北京到上海的延迟大约为30ms，上海到杭州的延迟约为5ms。假设有2个节点同时发生故障，每个机器发生故障的概率相等，且均为独立事件。

如果此时采用原先7节点，上海3节点、杭州2节点、北京2节点的部署模式，至少需要大多数节点确认，也就是4个节点才能完成选举和写入。如果宕机的2个节点都处于上海或者杭州数据中心，则系统的预期响应时间会从5ms上升到30-40ms。其中2个节点均处于上海和杭州的概率为
$$
A_5^2 / A_7^2 = 0.476
$$


也就是说系统有47.6%的概率延迟会上升6倍以上。对于两地三中心的部署，则上升比例更加显著，可能达到几十倍以上。

如果采用本文所述的Quorum策略，采用每个数据中心各3个节点部署，至少需要4个节点确认系统就可以进行选举和写入。当故障的2个节点同时处于上海或者杭州时，则系统必须等待北京数据中心的响应，则概率为
$$
A_2^1 * A_3^2 / A_9^2= 0.167
$$
如果同样是9节点部署，但是采取现有技术的策略，即至少5个节点进行确认，则需要跨地域访问北京数据中心的概率为
$$
A_5^2 / A_9^2 = 0.417
$$
可以看出本方案提出的Quorum策略对比原方案在9节点和7节点两种模式，在2个节点故障的情况下，需要访问北京数据中心的概率分别降低了60%、65%。

# 4 可用性

显然一个方案的收益不可能凭空产生，那么我们的Quorum策略的劣势在哪里呢？这是一个牺牲了可用性来保证延迟的策略。

一个允许大多数节点宕机的Quorum策略，可用性竟然低于一个只允许少部分节点宕机的Quorum策略，听上去可能比较反直觉。但实际上通过理论计算我们能很容易的发现这一点。

假设单个节点故障的概率为p，且节点故障均为独立事件，则n个节点中恰好存活m个的概率为:
$$
C_n^m(1-p)^mp^{n-m}
$$
我们将raft、paxos、zab等协议中采用的需要多数节点存活才能工作的策略称为多数派Quorum策略，将本文提出的通过分组来允许大多数节点失效的策略称之为分组Quorum策略。

## 4.1 多数派 Quorum 可用性

对于多数派的Quorum，n个节点至少要存活n/2+1个才可用。于是系统整体的可用性可以很方便的计算出来：
$$
\sum_{m=n/2+1}^{n}C_n^m(1-p)^mp^{n-m}
$$

## 4.2 分组Quorum可用性

对于本文提出的只需要分组Quorum，可用性计算要根据定义可以得出以下结论：

*  如果一个分组大多数节点可用，则这个分组可用
* 如果大多数分组可用，则分组Quorum策略可以正常工作

首先我们来计算每个分组的可用性，假设一个分组有i个节点，则根据4.1，可用性很显然为
$$
\sum_{j=i/2+1}^{i}C_i^j(1-p)^jp^{i-j}
$$
那么对于集群整体的可用性，假设有n个分组，则可用率为
$$
\sum_{m=n/2+1}^{n}C_n^m(\sum_{j=i/2+1}^{i}C_i^j(1-p)^jp^{i-j})^m(1-\sum_{j=i/2+1}^{i}C_i^j(1-p)^jp^{i-j})^{n-m}
$$

## 4.3 对比

对于我们上文所举的示例，代入如下数据计算可用性来做对比：

* 单节点故障率约为0.01，即为平均一年下线3天左右（故障加硬件维修/替换时间）
* 总结点数量9个，3个分组，每个分组3个节点

对于9节点的多数派Quorum，可用性为
$$
\sum_{m=5}^{9}C_9^m(0.99)^m(0.01)^{9-m} = 0.99999998781
$$
可以看到对于9节点，可用性达到了7个9。

对于分组Quorum策略的可用性，我们先计算单个分组的可用性
$$
\sum_{j=2}^{3}C_3^j(0.99)^j（0.01）^{3-j} = 1-C_3^1(0.99)（0.01）^2 = 0.999703
$$
则带入计算整个集群的可用性
$$
1-C_3^1(0.999703)（0.000297）^2 = 0.999999735451594
$$
可以看到采用分组Quorum的策略后，系统可用性从7个9下降到了6个9。

之所以出现这种允许故障的节点增加，但是可用性反而下降的情况，主要是因为分组以后，可以认为从第二个单个节点故障开始，对于整个系统的影响不再是一个孤立的事件，而是受之前故障节点分布的影响。相当于在任意一个节点故障后，剩余的节点对于可用性的影响已经从对等的角色，变成了带权重的角色。

回过头来看7节点时采用多数派的Quorum整个系统的可用性，不难计算得到
$$
\sum_{m=4}^{7}C_7^m(0.99)^m(0.01)^{9-m} = 0.999999658
$$
7节点下多数派的Quorum的可用性与9节点分组Quorum的可用性处于同一个数量级，都为6个9左右。

# 5 总结

本文提出了一种Quorum策略，允许集群内大多数节点故障，并在多节点故障时显著减少高延迟的概率。特别适用于元数据集群这种规模有限的场景，由于元数据集群对于大规模部署的存储业务数据的集群，数据规模可控，不会大幅度增长。所以只需要有限的硬件成本，即可在保证可用性的情况下大幅度降低多节点故障对于延迟的影响。