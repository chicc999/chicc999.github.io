---
title: FMQ之主从数据同步
date: 2018-11-05 16:45:00
tags:
- java
- 消息队列
categories: 编程
comments: true
---
*FMQ每组broker分为主从两个角色，支持一主多从。本文主要比较常见消息队列的主从复制逻辑，并且重新设计FMQ的复制逻辑。*
<!--more-->


# 1. 他山之石

我们先从市场上开源的消息队列看起，看看他们是如何来设计主从复制的模块。RocketMQ版本为4.2.0-incubating，kafka版本为0.10.1。本文对于具体的实现类，只对核心代码进行分析。

## 1.1 RocketMQ




## 1.2 Kafka

# 2. 现有方案

![FMQ 2.2.2](http://cyblog.oss-cn-hangzhou.aliyuncs.com/mqReplication/FMQ%20v2.2.2%20replicate1.png)

模型存在的问题如下：

* slaveReplica给masterReplica回复ack时还没有写盘成功，并不能保证复制成功
* masterReplica给slaveReplica发送复制请求时等收到ack后才会发送下一条，对于高延迟机房性能很差。
* 生产线程await(上图步骤4)等待写盘和复制结果，浪费了线程资源

实现存在的问题：

* 复制中状态太多，逻辑复杂


# 3. 新方案


## 3.1 目标

### 3.1.1 关键目标

* 为了支持异地多活，对多次传输下异地跨机房复制的延迟需要重点考虑。
* 强同步模型，一旦配置成同步复制模式，必须等待复制完成或者超时。

## 3.1.2 次要目标

* 支持多副本，多台slave
* 优化线程模型，生产线程不再等待
* 快速失败,连接异常或者slave不可用等能及时被master应用层感知

## 3.2 总体方案

新复制方案的各模块交互时序图如下所示：


![FMQ 2.2.3](http://cyblog.oss-cn-hangzhou.aliyuncs.com/mqReplication/FMQ%20v2.2.3%20Replicate.png)

### 3.2.1 生产模型

如方案中步骤1所示，putMessageHandler在接收到生产者的请求以后：

* 调用store模块将消息写进内存映射文件，更新wrtitePosition
* 通知复制模块有新消息到来

在此之后，putMessageHandler的生命周期就宣告结束，不再等待生产结果的返回。而存储模块store在写进内存文件以后，会将请求保存到复制管理器中。由于是顺序刷盘，所以消息被按照在日志中的顺序保存在replicationManager中。

由于可靠性要求，store模块必须强制进行刷盘，并在写盘成功后通知将刷盘位置通知给replicationManager。在实际实现中此处采用组提交技术，即刷盘线程不对每个请求刷盘，而是直接批量写入以后刷盘，只进行一次IO。


### 3.2.2 master复制模型

### 3.2.2.1 write

HAmaster的写出服务可能处于2个状态，inSync和outOfSync。

* outOfSync：复制模块处于未完全同步状态，即发现日志的writePostion大于slave已经复制的位置时，会持续的将日志写出。
* inSync：此时代表主从日志已经完全一致，此时写服务等待putMessageHandler给予有新消息需要同步的信号量。

无论出于哪种模式，均为异步写出，且不用slave回复响应。以此做到连续写入，避免异地网络下的高延迟。

### 3.2.2.2 read

HAmaster的读模块收到slave上报的刷盘位置后，更新内存里保存的slave的复制位置，并且通知replicationManager模块从节点的复制位置已经更新。

### 3.2.3 slave复制模型

### 3.2.3.1 read

slave收到master发送过来的日志以后，根据自身的策略进行刷盘操作。逻辑同master。

### 3.2.3.2 write

由于底层采用tcp协议保证了日志的顺序到达，所以多个响应只用回复最后刷盘的位置。由此slave并不主动对master的每一次请求进行相应。此处同样采用组提交技术，只有每次批量刷盘成功(对异步写盘是append成功)，才会回复一个相应表明写盘位置。

### 3.2.3 副本管理模块

* replicationManager中维护了全局的水位信息，每个slave复制位置更新，此模块都会得到通知。
* 水位如何更新取决于具体的策略。第一个版本暂时采用至少保证一个副本更新成功。
* 由于store模块是根据刷盘位置将请求顺序提交到本模块，所以在水位更新以后，从前至后，将处于水位线以下的请求全部回复。


## 3.3 特殊处理的场景

* slave所请求复制的日志已经被master删除。
* 由于刷盘和复制同时进行，复制成功时未写盘成功，master重启后未写盘部分丢失，主从不一致。
* 由于异步复制，当网络故障但是应用层未发现时，会持续不断写出，堆积在本地堆外内存，有把内存用尽的风险。需要加上滑动窗口的概念或者进行限流。
* 主从无数据时的定时心跳。
* 主从复制网络发生故障时，迅速被应用层感知并回复的方案。
* 新的slave无数据启动时复制太快打满网络，影响正常业务。

# 4 实现

## 4.1 启动

### 4.1 slave启动

* connect

slave主动发送请求的触发事件如下：

* CONNECT
* RECONNECT
* SYN_ACK_OFFSET
* GET_OFFSET

HAStatus：

* NONE
* CONNECTED



# 4 测试方案及结果

## 4.1 测试方案

在重写复制模块之前，为了验证效果，必须要有横向和纵向的性能测试，比较FMQ的新版本和老版本、kafka、RocketMQ之间的性能差异。

### 4.1.1 硬件需求

| 物理机数量| cpu |内存|磁盘|带宽|
|:----------:|:-------------:|:-------------:|:-------------:|:-------------:|
|4|48|256G|2T|1000M|

### 4.1.2 环境版本

* JDK8
* 换G1搜集器做纵向测试（对比FMQ 2.2.2和2.2.3）

### 4.1.3 软件版本

* FMQ 2.2.2
* RocketMQ 4.2.0-incubating
* kafka 0.10.1.0

### 4.1.4 测试指标

#### 机器指标

* cpu使用率
* 内存使用率
* 磁盘使用率
* 磁盘写速度
* 磁盘度速度
* 磁盘I/O占比
* 磁盘I/O等待时间
* 网络IO流量

#### 虚拟机指标

* 堆内存占用
* 堆外内存占用
* FGC次数
* FGC消耗时间
* YGC次数
* YGC消耗时间

#### 业务指标

* TPS
* 正确率
* 平均响应时间
* TP99
* TP999

### 4.1.5 测试场景

* 主题数量分别为1、8、16、64、128、256
* 消息大小分别为100、1K、10K
* 分别测试只有生产者和既生产又消费
* 分别测试同步写盘同步复制/异步写盘异步复制/同步写盘异步复制的场景
* 关闭内存映射文件或者模拟内存映射文件可用内存耗尽，重复以上测试

## 4.2 测试结果



# 1. 启动

# 2. 数据复制

## 2.1 日志复制

* slave和master成功建立连接。
* slave向master发送getOffset请求（请求中包含本地日志文件的最大偏移量）。
* master根据slave发送过来的最大偏移量，返回一个合适的日志复制起始位置。master回复response包含起始复制offset和日志最大偏移量journalMaxOffset。

## 2.1 索引日志重建

主从同步时只会对日志文件和消费者索引文件同步，而不会对topic的队列索引文件同步。队列索引文件可以通过日志文件进行重建，所以在数据复制过程中，还伴随索引重建的过程。


