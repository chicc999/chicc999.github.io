---
title: JPacificA - 一种跨域线性一致性算法
date: 2021-08-10 10:33:28
tags:
- 一致性算法
- 消息队列
categories: 编程
comments: true
---

*本文介绍了一种可以保证跨域线性一致性的算法，用来解决目前跨域数据灾备的问题。*
<!--more-->

# 1 介绍

面对不可避免的硬件故障率，数据密集型系统通常采用多个数据副本放在多个独立的硬件资源上来进行容灾。然而多个数据副本之间如何保证完全一致则是一个不小的挑战。

同时当要考虑地域级的数据容灾时，跨地域的高延迟往往给对局域网设计的一致性算法带来一些意想不到问题。如选举达成一致的时间过长、选举后的主节点在地域间漂移、频繁触发选举等。本文基于微软发表的一致性协议PacificA<sup>【1】</sup>，提出了一种可以支持跨域线性一致性的算法。

# 2 背景

## 2.1需求及现状

在没有引入本文所述的算法之前，金融级消息队列（FMQ）主要采用单分片主备、多分片多活模式。即对于每份数据会被写入一个分片，每个分片有主从两个节点（或多个从）负责多副本。当分片发生故障时，则将写操作偏移到其他分片，原有分片只负责历史数据的读取（类似RocketMQ）。

随着内部产品的迭代及输出，以及单元化的推进，内外部用户对我们的系统提出了很多当时不满足的需求。功能层面主要集中在以下几点：

1. 提供对于消息严格顺序的支持
2. 提供对于数据跨域容灾的支持，即跨地域进行数据同步
3. 提供跨可用区/跨域的数据可靠性保证
4. 选举后主节点稳定，满足单元化就近读写的需求

从技术层面翻一下这些需求，其实1就是要提供至少线性一致性的读写（如果考虑到是消息顺序一致性也能满足）；2和3则要求在跨可用区/地域上提供线性一致性的读写；4则要求选举结果必须在一个可以预先设定的范围内。

从成本方面考虑，则主要有以下需求：

* 提供尽量灵活的副本策略

  很多业务，根据重要性以及数据的特点，对可靠性的要求并不一样；对于用户来说灵活的副本数量可以节省大量的资源

* 使用尽量少的资源即能满足跨可用区/跨地域容灾

  在实际输出的场景中，很多外部用户并不存在两地三中心，两地两中心是一种很常见的情况。所以在我们的成本考量上，希望能尽量满足用较少的资源就进行跨可用区/跨地域的容灾

由于跨可用区可以作为一种跨地域的特殊情况（同地域RTT较小），既然我们的目标是设计一个支持跨地域线性一致的算法，那么跨可用区自然也支持。所以在下文中，不再额外讨论跨可用区的问题。

## 2.2 业内方案

对于跨域传输，主要有两种方案。一种是将集群直接跨地域部署，如在一个三中心架构中，节点按照2+2+1进行部署；另一种则是每个中心一个集群，然后依靠工具进行跨集群的数据同步。

### 2.2.1 集群跨域部署

![muti-region](/Users/chenyun5/Documents/JPacificA/muti-region.png)

对于一个集群的多个节点，直接跨地域进行部署，数据走一致性协议。跨集群部署主要问题如下：

1. 写入响应时间长、选举时间长

   地域之间的延迟一般在几十毫秒到几百毫秒，跨地域部署意味着写入和选举达成一致的时间会变得很长

2. 主节点所在地域不稳定

   跨地域部署意味着主节点在选举后可能会变成集群内任意地域的任意节点,意味着大量本身做了就近访问的业务需要跨地域进行数据访问

### 2.2.2 跨集群数据同步

这种方式是指在异地各自搭建一个集群，然后依靠外部工具或内置的机制做集群间数据复制，如pulsar和阿里的drc。



![pulsar](/Users/chenyun5/Documents/JPacificA/pulsar.png)

如上图，pulsar的跨域复制实质上是Cluster-A、Cluster-B、Cluster-C三个集群，依靠内置的生产者、消费者做数据同步。

#### ![drc](/Users/chenyun5/Documents/JPacificA/drc.png)

阿里的跨地域复制如上图，依赖外部的otter组件进行同步。

跨地域进行集群间数据同步主要存在如下问题：

1. 占用资源过多

   例如两个集群如果都采用三副本，则意味着这种模式下同一份数据被保存了6个副本，十分浪费存储资源

2. 需要引入外部服务或者机制

   两个集群间进行数据复制则必然需要引入集群以外的组件，可以是系统本身提供的或者外部服务提供的。这些外部服务的工作需要消耗一定的资源

3. 集群间无法保证一致性

   既然是跨集群数据同步，意味着同一份数据在两个集群实际上是独立的两个逻辑实体，只是内容相同。通过外部服务进行数据同步，两个集群之间本身没有没有任何关系，自然无法有任何一致性的保证

   在接下来的章节，本文提出了一种保证跨域线性一致性的算法来解决此问题。

# 3 一种跨域线性一致性算法

## 3.1 日志复制

对于一个集群，我们所有的请求都由主节点处理。对于读请求直接返回结果，对于写请求，则必须通过日志复制将请求复制到从节点，然后重放到状态机。日志复制流程如下：

* 主节点为每个写请求分配一个单调递增的序号，并写入日志文件。这里我们直接用请求写入主节点顺序自增的日志偏移量表示序号，即entryLogOffset
* 写入完成后，主节点发送AppendEntry请求，其中包括的数据结构除了日志以外，还包括主节点当前任期(term)、日志起始位置（entryLogOffset）、已经提交的位置（commitOffset）以及checksum。对比其他一致性协议此处追加了应用层的checksum，只是为了保证数据准确，如果网络整个链路每一跳都进行了crc校验，则可以忽略，否则无法保证端到端的正确性<sup>【2】</sup>。
* 从节点收到AppendEntry后，首先对任期进行校验，如果任期相同，再进行写入位置校验（采用单个长连接严格顺序复制，如果改成类似paxos支持乱序复制则无需做此校验）。最后checksum校验通过，则持久化到本地磁盘，并且将日志的结束位置返回给主节点。
* 主节点收到从节点更新的复制位置后，取所有当前任期从节点已经复制的最小位置作为commitOffset；对commitOffset之前的日志都可以apply到状态机，然后响应给客户端；同时在下一个心跳或者appendEntry将commitOffset同步到所有从节点

**日志完备性：**

下图展示了一个一主两从的场景，commitOffset为真正可以apply到状态机和进行响应客户端的位置，slave commitOffset为从节点感知到的commitOffset(可能落后于真正的commitOffset)。由于commitOffset是取的所有从节点都已经写成功的位置，所以当从节点在新的任期被选为主时，只要不进行日志截断，就能保证其日志中一定含有commitOffset之前所有的日志，即commitOffset的所有日志都是安全的。

![image-20210811173151689](/Users/chenyun5/Library/Application Support/typora-user-images/image-20210811173151689.png)

## 3.2 配置管理

在JPacificA的设计中，需要有一个全局的配置管理模块。对于每个复制组，配置管理模块记录了当前集群的选举结果以及当前结果对应的任期。

配置管理模块需要提供带cas语义的更新，一般采用paxos的一种实现，如zookeeper、etcd或者自研的raft、muti-paxos、epaxos等实现。

一个节点在错误检测机制下，怀疑其它节点故障时（主怀疑某个从故障或者从怀疑主故障），会生成新一轮任期的配置信息，以及将原配置信息的任期号加一作为新一轮的任期，尝试更新到配置管理集群。如果任期号匹配（即cas更新成功），则发布配置成功，相当于成功当选；如果发布失败，则读取此时的配置，并更新本地的配置和任期。

当发生网络分区时，可能发生多个节点同时尝试变更配置的操作。例如主节点试图把失联的从节点剔除出同步副本列表；而同时从节点试图将自己选为主，并将原来的主节点剔除出同步副本列表。配置管理集群将会接收第一个符合cas更新的请求，并拒绝剩下的请求（任期号不再匹配）。

**主节点唯一性：**

在任何时候，配置管理器的cas语义都保证在同一个任期只有一份配置，配置里的主节点就是这个任期实际的主节点。所以在任意一个任期，我们可以确定主节点的唯一性。

## 3.3 租约与去中心化

虽然我们可以保证在一个任期在配置上只有一个主节点，但是却没法避免发生分区时老的主节点依然对外提供服务。

对于老的主节点来说，由于写成功必须要当前任期内所有从节点写成功才响应，而新的主节点又必然是从节点才能当选，所以两个集合存在交集，即老的主节点写操作总是无法成功。但是对于读请求，则会出现双主的情况。

为了解决这个问题，我们引入租约机制。主节点周期性（heartbeat period）的向所有从节点获取租约，如果经过一定周期（lease period）没有获得某个从节点的租约，则主节点停止进行请求的处理。然后主节点会将此从节点从同步副本集合中剔除，并且更新此配置到配置管理节点，开启下一个任期。

同样，如果一个从节点在一定周期（grace period）内没有收到主节点的租约请求，则会发起选举，将自己在下一任期的角色变更为主节点，尝试向配置管理节点更新。

为了保证发生分区时主节点相对稳定，即优先由主节点将断联的从节点剔除，我们一般会保证 [grace period] >  [lease period]  > [heartbeat period] *2。

同时由于[lease period] < [grace period]，所以我们可以保证在从节点发起选举时，原来的主节点一定已经放弃了自身的主节点身份或者已经进入了新一轮任期，由此保证**主节点唯一性**。当然如果系统可以允许脏读，此处的实现也可以原先主节点继续提供读服务，牺牲一致性来保证可用性。

不同于其他中心化的租约设计（如GFS）需要和协调节点（如我们的配置管理集群）通信,本方案的租约是在集群内部，即去中心化的租约。这个设计主要是考虑2点：

* 当集群线性增长时，中心节点不会有压力
* 当中心节点的集群不可用时，只要不发生选举，不影响数据节点的可用性

## 3.4 日志匹配

在新的主节点被选举出来后，因为所有从节点都有参与选举的资格，所以未必能保证新的主节点日志写入位置最大。如图所示：

![reconcile](/Users/chenyun5/Documents/JPacificA/reconcile.jpg)

图上展示了A、B、C、D四个日志副本。每条日志假设长度为1，记录了此条日志被添加的任期号和具体的操作。在某个时刻，原来在任期3是master角色的A副本所在节点宕机了，这时候commitOffset在偏移量为5的位置，B、C、D都在当前任期的同步副本集合中，所以都有资格成为新任期的主节点。假如此时节点B被选为了任期4的主节点，则对于D副本需要追上日志，而对于B和重新活过来的A则需要截断日志。

日志匹配一般有以下2种做法：

* 方案一：恢复/协调阶段slave上报自己日志的最后一条，然后从最后一条开始向前逐条与master的同位置日志比较，直到找到完全匹配的日志（任期、位置以及数据crc），并将此作为日志起始位置。slave中原先大于此位置的日志全部截断。当然此种方法有个优化是可以将逐条比较改为二分比较。
* 方案二：每个slave定时做checkpoint，记录commitOffset，为了与真实的commitOffset比较，我们称之为checkPointCommitOffset；内存里收到master同步的commitOffset但是还没来得及持久化部分称为slaveCommitOffset。由于存在checkPointCommitOffset <= slaveCommitOffset <= commitOffset <= logEndOffset，所以在日志截断到checkPointCommitOffset后，可以直接从这个位置去复制日志。但是此方案带来两个问题：
  * 状态机需要也从checkPoint点进行恢复，而恢复时间可能会相对较长
  * 由于进行的日志截断位置小于commitOffset，日志完备性不再保证；为了保证正确性，从节点在协调过程中需要被踢出同步副本，此时如果新的主节点宕机，将导致整个系统不可用（没有可供参与选举的副本）

方案二明显是个不可用的方案，但是结合方案一，却为我们缩小搜索范围加快恢复提供了思路。

我们将恢复阶段的逐条对比改为从checkPointCommitOffset 到logEndOffset的二分查找，直到找到某条日志是匹配的（此条日志一定大于等于真正的commitOffset），而下一条不匹配，则可以确定slave要开始复制的位置。特别的，如果每个entry带上前一条日志的term和logOffset，我们可以通过单个entry来判断是否匹配，进一步缩短恢复时间。

## 3.5 跨域复制

通过3.1-3.4我们描述了一种线性一致性算法。但是此算法并不适合做跨地域的部署。主要原因如下:

* 跨地域的网络不稳定，可能会导致频繁的选举
* 跨地域延迟(Round-Trip Time)较高，为了满足 [lease period] > [grace period]  > [heartbeat period] *2 >> [Round-Trip Time],会导致lease period过大，则发生异常选举时间（不可用时间）变长，SLA下降
* 主节点不稳定，无法提供单元化的服务

针对以上数据中心内跨域一致性算法扩展，我们添加了不参与选举的观察者（observer）节点，来解决跨域复制的问题。其角色状态转换图如下：

![roleChange](/Users/chenyun5/Documents/JPacificA/roleChange.png)

其数据流向如图所示：

![dataFlow](/Users/chenyun5/Documents/JPacificA/dataFlow.jpg)

如图所示，在本方案中，每个集群参与选举的节点只部署在一个数据中心内，以此保证选举后的结果稳定在同一个数据中心，以及保证选举的延迟相对稳定。跨数据中心则采用不参与选举的观察者节点进行数据同步。

对于客户端来说，正常情况下只对主节点在本数据中心内的集群进行写入和读取，而如果需要读取主节点在其他数据中心集群的数据，则对此集群在本数据中心的观察者进行访问。

## 3.6 跨域线性一致性保证

当我们采用上文所述的跨域复制技术后，存在一个新的问题，其线性一致性无法保证<sup>【3】</sup>。

例如我们假设集群1和2处于不同地域，有客户端client 1操作本地集群1设置x的值后读取y的值，client 2操作本地集群2设置y的值后读取x的值。初始x=0，y=0则有

![consensus](/Users/chenyun5/Documents/JPacificA/consensus.png)

client 1: x=0 -> x=5 -> y=0 -> y=3

client 2: y=0 -> y=3 -> x=0 -> x=5

client 1看到x=5成功后去observer上读y，假设此时client 2 set y=3已经返回，则client1依然可能读不到最新的数据3，因此不满足线性一致性。

我们通过响应模型，将用户一致性视图与集群一致性视图分离的方法来解决这个问题，如下图所示：

![respModel](/Users/chenyun5/Documents/JPacificA/respModel.png)

三种响应模型分别为：

* NO ack:不需要任何副本写成功即能进行响应

* ISR ack:所有在当前任期有资格参与选举的副本都必须写成功才能进行ack

* ALL ack:所有副本都写成功才能进行ack

本方法通过引入全局副本都写成功才能响应的ALL ack，为用户在集群内提供了线性一致的视图。以上文举例，当client 1去读y时，如果client 2已经set y=3成功且是All ack，则意味着本数据中心的观察者y的数据也是3，所以读到的y必然是3；如果client 2的set操作还未返回，则client 1对y的读操作和client2的对y的set操作是一个并发操作，读到哪个值都不破坏一致性。

# 4 应用

我们将此算法应用在了金融级分布式消息队列（FMQ）的单元化场景中，以及帮助其它中间件解决异地多活的问题。

下面是一个典型的两地三中心的例子。我们假设有不同地域regionA和RegionB，同时RegionA有2个可用区。以下均为数据单向流动举例，双向的话需要对等部署集群。

* 红色的图例代表需要异地多活的业务，需要将数据传输到异地，但是可以接受区域故障时，异地机房有少量的数据丢失。我们选择参与选举的节点部署在本地的2个机房，跨地域部署一个观察者。
* 蓝色的图例代表需要保证异地线性一致性的业务，即区域故障时严格不丢数据。这种业务采用和红色图例一样的部署模式，但是会将响应级别调整成ALL ack
* 紫色的图例代表单元化的需求。即对于这部分应用来说，必须保证选举前后的主节点稳定在同一个单元内。假设我们将一个可用区划分为一个单元，则将参与选举的节点部署在同一个可用区，同时将观察者部署在其它单元。响应模型可以根据需要选择ISR ack或者ALL ack
* 绿色的图例则代表普通的需要跨可用区容灾的业务，我们将选举节点部署在同城的两个可用区内。

![deploy](/Users/chenyun5/Documents/JPacificA/deploy.jpg)

# 5 性能

最后来看下JPacificA的吞吐量问题。测试总共使用了3个服务器，硬件如下：

|              | 参数（标N）                                                  |
| ------------ | ------------------------------------------------------------ |
| CPU          | Intel(R)  Xeon(R) CPU E5-2650 v4 @ 2.20GHz (48线程)          |
| MEM          | 250G                                                         |
| 网卡         | Intel  Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection（2块） |
| 硬盘         | 2.1T  （SSD）                                                |
| 本地网络延迟 | 0.1ms                                                        |
| 异地网络延迟 | 17-20ms（北京-宿迁）                                         |
| payload      | 1024B                                                        |
| 发送模式     | 客户端单条发送，不做批量                                     |

![throughput](/Users/chenyun5/Documents/JPacificA/throughput.jpg)

由以上数据可以得到如下结论

* 单分片吞吐量30w+
* 出口带宽不是瓶颈的情况下，异地观察者对于吞吐量及响应时间无影响
* 对于ALL ack，异地网络固定延迟只影响响应时间，对吞吐量几乎没影响
* 对于ALL ack，达到相同吞吐量需要更多的并发（平均响应时间更长）

# 6 总结

对于基于复制状态机来构建的分布式系统，一致性算法是最重要的组成部分，其正确性、可用性、吞吐量对于整个系统的构建起着重要作用。本文提供了一种可以在跨域的情况下，满足线性一致性的共识算法，来建立跨地域的数据系统。并且针对实际场景，给出了本算法在实际系统中的应用情况以及吞吐量信息。

# 7 引用

【1】Lin W, Yang M, Zhang L, et al. PacificA: Replication in log-based distributed storage systems[J]. 2008.

【2】Saltzer J H, Reed D P, Clark D D. End-to-end arguments in system design[J]. ACM Transactions on Computer Systems (TOCS), 1984, 2(4): 277-288.

【3】Lev-Ari K, Bortnikov E, Keidar I, et al. Modular composition of coordination services[C]//2016 {USENIX} Annual Technical Conference ({USENIX}{ATC} 16). 2016: 251-264.